{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/forrestbao/pyeeg.git\n",
      "  Cloning https://github.com/forrestbao/pyeeg.git to c:\\users\\faizan\\appdata\\local\\temp\\pip-req-build-k3x1tf5l\n",
      "Requirement already satisfied: numpy>=1.9.2 in c:\\users\\faizan\\anaconda3\\lib\\site-packages (from pyeeg==0.4.4) (1.16.2)\n",
      "Building wheels for collected packages: pyeeg\n",
      "  Building wheel for pyeeg (setup.py): started\n",
      "  Building wheel for pyeeg (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\faizan\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-yttppi6f\\wheels\\2d\\3f\\ad\\106d4fc80b61d1ea1fc18e76e7439fd98aa043d83d58eae741\n",
      "Successfully built pyeeg\n",
      "Installing collected packages: pyeeg\n",
      "Successfully installed pyeeg-0.4.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://github.com/forrestbao/pyeeg.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyeeg as pe\n",
    "import pickle as pickle\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import os\n",
    "#import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel = [1,2,3,4,6,11,13,17,19,20,21,25,29,31] #14 Channels chosen to fit Emotiv Epoch+\n",
    "band = [4,8,12,16,25,45] #5 bands\n",
    "window_size = 256 #Averaging band power of 2 sec\n",
    "step_size = 16 #Each 0.125 sec update once\n",
    "sample_rate = 128 #Sampling rate of 128 Hz\n",
    "subjectList = ['01','02','03']\n",
    "#List of subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FFT_Processing (sub, channel, band, window_size, step_size, sample_rate):\n",
    "    '''\n",
    "    arguments:  string subject\n",
    "                list channel indice\n",
    "                list band\n",
    "                int window size for FFT\n",
    "                int step size for FFT\n",
    "                int sample rate for FFT\n",
    "    return:     void\n",
    "    '''\n",
    "    meta = []\n",
    "    with open('C:/Users/faizan/Downloads/data_preprocessed_python/data_preprocessed_python/s' + sub + '.dat', 'rb') as file:\n",
    "\n",
    "        subject = pickle.load(file, encoding='latin1') #resolve the python 2 data problem by encoding : latin1\n",
    "\n",
    "        for i in range (0,40):\n",
    "            # loop over 0-39 trails\n",
    "            data = subject[\"data\"][i]\n",
    "            labels = subject[\"labels\"][i]\n",
    "            start = 0;\n",
    "\n",
    "            while start + window_size < data.shape[1]:\n",
    "                meta_array = []\n",
    "                meta_data = [] #meta vector for analysis\n",
    "                for j in channel:\n",
    "                    X = data[j][start : start + window_size] #Slice raw data over 2 sec, at interval of 0.125 sec\n",
    "                    Y = pe.bin_power(X, band, sample_rate) #FFT over 2 sec of channel j, in seq of theta, alpha, low beta, high beta, gamma\n",
    "                    meta_data = meta_data + list(Y[0])\n",
    "\n",
    "                meta_array.append(np.array(meta_data))\n",
    "                meta_array.append(labels)\n",
    "\n",
    "                meta.append(np.array(meta_array))    \n",
    "                start = start + step_size\n",
    "                \n",
    "        meta = np.array(meta)\n",
    "        np.save('C:/Users/faizan/Downloads/data_preprocessed_python/data_preprocessed_python/s' + sub, meta, allow_pickle=True, fix_imports=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subjects in subjectList:\n",
    "    FFT_Processing (subjects, channel, band, window_size, step_size, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset: (51240, 70) (51240, 4)\n",
      "testing dataset: (7320, 70) (7320, 4)\n"
     ]
    }
   ],
   "source": [
    "data_training = []\n",
    "label_training = []\n",
    "data_testing = []\n",
    "label_testing = []\n",
    "\n",
    "for subjects in subjectList:\n",
    "\n",
    "    with open('C:/Users/faizan/Downloads/data_preprocessed_python/data_preprocessed_python/s' + subjects + '.npy', 'rb') as file:\n",
    "        sub = np.load(file)\n",
    "        for i in range (0,sub.shape[0]):\n",
    "            if i % 8 == 0:\n",
    "                data_testing.append(sub[i][0])\n",
    "                label_testing.append(sub[i][1])\n",
    "            else:\n",
    "                data_training.append(sub[i][0])\n",
    "                label_training.append(sub[i][1])\n",
    "           \n",
    "np.save('C:/Users/faizan/Downloads/data_preprocessed_python/data_preprocessed_python/data_training', np.array(data_training), allow_pickle=True, fix_imports=True)\n",
    "np.save('C:/Users/faizan/Downloads/data_preprocessed_python/data_preprocessed_python/label_training', np.array(label_training), allow_pickle=True, fix_imports=True)\n",
    "print(\"training dataset:\", np.array(data_training).shape, np.array(label_training).shape)\n",
    "\n",
    "np.save('C:/Users/faizan/Downloads/data_preprocessed_python/data_preprocessed_python/data_testing', np.array(data_testing), allow_pickle=True, fix_imports=True)\n",
    "np.save('C:/Users/faizan/Downloads/data_preprocessed_python/data_preprocessed_python/label_testing', np.array(label_testing), allow_pickle=True, fix_imports=True)\n",
    "print(\"testing dataset:\", np.array(data_testing).shape, np.array(label_testing).shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:/Users/faizan/Downloads/data_preprocessed_python/data_preprocessed_python/data_training.npy', 'rb') as fileTrain:\n",
    "    X  = np.load(fileTrain)\n",
    "    \n",
    "with open('C:/Users/faizan/Downloads/data_preprocessed_python/data_preprocessed_python/label_training.npy', 'rb') as fileTrainL:\n",
    "    Y  = np.load(fileTrainL)\n",
    "    \n",
    "X = normalize(X)\n",
    "Z = np.ravel(Y[:, [1]])\n",
    "\n",
    "Arousal_Train = np.ravel(Y[:, [0]])\n",
    "Valence_Train = np.ravel(Y[:, [1]])\n",
    "Domain_Train = np.ravel(Y[:, [2]])\n",
    "Like_Train = np.ravel(Y[:, [3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.utils import to_categorical \n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "import timeit\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Flatten, Dense, Dropout\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D, ZeroPadding1D\n",
    "from keras.optimizers import SGD\n",
    "#import cv2, numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "y_train = to_categorical(Z)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(X[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:/Users/faizan/Downloads/data_preprocessed_python/data_preprocessed_python/data_testing.npy', 'rb') as fileTrain:\n",
    "    M  = np.load(fileTrain)\n",
    "    \n",
    "with open('C:/Users/faizan/Downloads/data_preprocessed_python/data_preprocessed_python/label_testing.npy', 'rb') as fileTrainL:\n",
    "    N  = np.load(fileTrainL)\n",
    "\n",
    "M = normalize(M)\n",
    "L = np.ravel(N[:, [1]])\n",
    "\n",
    "Arousal_Test = np.ravel(N[:, [0]])\n",
    "Valence_Test = np.ravel(N[:, [1]])\n",
    "Domain_Test = np.ravel(N[:, [2]])\n",
    "Like_Test = np.ravel(N[:, [3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.array(M[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "y_test = to_categorical(L)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0],x_train.shape[1], 1)\n",
    "x_test = x_test.reshape(x_test.shape[0],x_test.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51240, 70, 1)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "num_classes = 10\n",
    "epochs = 200\n",
    "input_shape=(x_train.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70, 1)\n"
     ]
    }
   ],
   "source": [
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Convolution1D, ZeroPadding1D, MaxPooling1D, BatchNormalization, Activation, Dropout, Flatten, Dense\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_7 (Conv1D)            (None, 70, 128)           512       \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 70, 128)           512       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 35, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 35, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 35, 128)           512       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 17, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 2176)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 64)                139328    \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 10)                170       \n",
      "=================================================================\n",
      "Total params: 192,922\n",
      "Trainable params: 192,410\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "intput_shape=(x_train.shape[1], 1)\n",
    "model.add(Conv1D(128, kernel_size=3,padding = 'same',activation='relu', input_shape=input_shape))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=(2)))\n",
    "model.add(Conv1D(128,kernel_size=3,padding = 'same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=(2)))\n",
    "#model.add(Conv1D(64,kernel_size=3,padding = 'same', activation='relu'))\n",
    "#model.add(MaxPooling1D(pool_size=(2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='tanh'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32, activation='tanh'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "51240/51240 [==============================] - 69s 1ms/step - loss: 2.0102 - accuracy: 0.2650\n",
      "Epoch 2/200\n",
      "51240/51240 [==============================] - 72s 1ms/step - loss: 1.7687 - accuracy: 0.3254\n",
      "Epoch 3/200\n",
      "51240/51240 [==============================] - 67s 1ms/step - loss: 1.6625 - accuracy: 0.3671\n",
      "Epoch 4/200\n",
      "51240/51240 [==============================] - 59s 1ms/step - loss: 1.5719 - accuracy: 0.4089\n",
      "Epoch 5/200\n",
      "51240/51240 [==============================] - 61s 1ms/step - loss: 1.5110 - accuracy: 0.4345\n",
      "Epoch 6/200\n",
      "51240/51240 [==============================] - 58s 1ms/step - loss: 1.4653 - accuracy: 0.4547\n",
      "Epoch 7/200\n",
      "51240/51240 [==============================] - 57s 1ms/step - loss: 1.4126 - accuracy: 0.4731\n",
      "Epoch 8/200\n",
      "51240/51240 [==============================] - 57s 1ms/step - loss: 1.3686 - accuracy: 0.4884\n",
      "Epoch 9/200\n",
      "51240/51240 [==============================] - 59s 1ms/step - loss: 1.3357 - accuracy: 0.5065\n",
      "Epoch 10/200\n",
      "51240/51240 [==============================] - 57s 1ms/step - loss: 1.2997 - accuracy: 0.5186\n",
      "Epoch 11/200\n",
      "51240/51240 [==============================] - 61s 1ms/step - loss: 1.2654 - accuracy: 0.5365\n",
      "Epoch 12/200\n",
      "51240/51240 [==============================] - 57s 1ms/step - loss: 1.2421 - accuracy: 0.5465\n",
      "Epoch 13/200\n",
      "51240/51240 [==============================] - 57s 1ms/step - loss: 1.2192 - accuracy: 0.5528\n",
      "Epoch 14/200\n",
      "51240/51240 [==============================] - 57s 1ms/step - loss: 1.1848 - accuracy: 0.5688\n",
      "Epoch 15/200\n",
      "51240/51240 [==============================] - 58s 1ms/step - loss: 1.1670 - accuracy: 0.5774\n",
      "Epoch 16/200\n",
      "51240/51240 [==============================] - 61s 1ms/step - loss: 1.1465 - accuracy: 0.5850\n",
      "Epoch 17/200\n",
      "51240/51240 [==============================] - 57s 1ms/step - loss: 1.1193 - accuracy: 0.5957\n",
      "Epoch 18/200\n",
      "51240/51240 [==============================] - 57s 1ms/step - loss: 1.1068 - accuracy: 0.6003\n",
      "Epoch 19/200\n",
      "51240/51240 [==============================] - 57s 1ms/step - loss: 1.0901 - accuracy: 0.6074\n",
      "Epoch 20/200\n",
      "51240/51240 [==============================] - 58s 1ms/step - loss: 1.0641 - accuracy: 0.6159\n",
      "Epoch 21/200\n",
      "51240/51240 [==============================] - 58s 1ms/step - loss: 1.0437 - accuracy: 0.6238\n",
      "Epoch 22/200\n",
      "51240/51240 [==============================] - 57s 1ms/step - loss: 1.0307 - accuracy: 0.6270\n",
      "Epoch 23/200\n",
      "51240/51240 [==============================] - 58s 1ms/step - loss: 1.0258 - accuracy: 0.6300\n",
      "Epoch 24/200\n",
      "51240/51240 [==============================] - 60s 1ms/step - loss: 1.0153 - accuracy: 0.6343\n",
      "Epoch 25/200\n",
      "51240/51240 [==============================] - 59s 1ms/step - loss: 1.0025 - accuracy: 0.6375\n",
      "Epoch 26/200\n",
      "51240/51240 [==============================] - 63s 1ms/step - loss: 0.9860 - accuracy: 0.6463\n",
      "Epoch 27/200\n",
      "51240/51240 [==============================] - 64s 1ms/step - loss: 0.9744 - accuracy: 0.6487\n",
      "Epoch 28/200\n",
      "51240/51240 [==============================] - 70s 1ms/step - loss: 0.9665 - accuracy: 0.6508\n",
      "Epoch 29/200\n",
      "51240/51240 [==============================] - 68s 1ms/step - loss: 0.9577 - accuracy: 0.6566\n",
      "Epoch 30/200\n",
      "51240/51240 [==============================] - 68s 1ms/step - loss: 0.9482 - accuracy: 0.6601\n",
      "Epoch 31/200\n",
      "51240/51240 [==============================] - 67s 1ms/step - loss: 0.9291 - accuracy: 0.6659\n",
      "Epoch 32/200\n",
      "51240/51240 [==============================] - 69s 1ms/step - loss: 0.9301 - accuracy: 0.6661\n",
      "Epoch 33/200\n",
      "51240/51240 [==============================] - 68s 1ms/step - loss: 0.9167 - accuracy: 0.6715\n",
      "Epoch 34/200\n",
      "51240/51240 [==============================] - 69s 1ms/step - loss: 0.9165 - accuracy: 0.6738\n",
      "Epoch 35/200\n",
      "51240/51240 [==============================] - 64s 1ms/step - loss: 0.9116 - accuracy: 0.6730\n",
      "Epoch 36/200\n",
      "51240/51240 [==============================] - 59s 1ms/step - loss: 0.8941 - accuracy: 0.6802\n",
      "Epoch 37/200\n",
      "51240/51240 [==============================] - 59s 1ms/step - loss: 0.8885 - accuracy: 0.6817\n",
      "Epoch 38/200\n",
      "51240/51240 [==============================] - 58s 1ms/step - loss: 0.8740 - accuracy: 0.6883\n",
      "Epoch 39/200\n",
      "51240/51240 [==============================] - 59s 1ms/step - loss: 0.8709 - accuracy: 0.6883\n",
      "Epoch 40/200\n",
      "51240/51240 [==============================] - 60s 1ms/step - loss: 0.8577 - accuracy: 0.6928\n",
      "Epoch 41/200\n",
      "51240/51240 [==============================] - 59s 1ms/step - loss: 0.8601 - accuracy: 0.6927\n",
      "Epoch 42/200\n",
      "51240/51240 [==============================] - 59s 1ms/step - loss: 0.8698 - accuracy: 0.6878\n",
      "Epoch 43/200\n",
      "51240/51240 [==============================] - 59s 1ms/step - loss: 0.8550 - accuracy: 0.6952\n",
      "Epoch 44/200\n",
      "51240/51240 [==============================] - 58s 1ms/step - loss: 0.8470 - accuracy: 0.6980\n",
      "Epoch 45/200\n",
      "51240/51240 [==============================] - 60s 1ms/step - loss: 0.8473 - accuracy: 0.6976\n",
      "Epoch 46/200\n",
      "51240/51240 [==============================] - 59s 1ms/step - loss: 0.8320 - accuracy: 0.7025\n",
      "Epoch 47/200\n",
      "51240/51240 [==============================] - 63s 1ms/step - loss: 0.8411 - accuracy: 0.6988\n",
      "Epoch 48/200\n",
      "51240/51240 [==============================] - 73s 1ms/step - loss: 0.8306 - accuracy: 0.7030\n",
      "Epoch 49/200\n",
      "51240/51240 [==============================] - 75s 1ms/step - loss: 0.8226 - accuracy: 0.7083\n",
      "Epoch 50/200\n",
      "51240/51240 [==============================] - 70s 1ms/step - loss: 0.8191 - accuracy: 0.7069\n",
      "Epoch 51/200\n",
      "51240/51240 [==============================] - 65s 1ms/step - loss: 0.8154 - accuracy: 0.7090\n",
      "Epoch 52/200\n",
      "51240/51240 [==============================] - 68s 1ms/step - loss: 0.8069 - accuracy: 0.7108\n",
      "Epoch 53/200\n",
      "51240/51240 [==============================] - 71s 1ms/step - loss: 0.8045 - accuracy: 0.7146\n",
      "Epoch 54/200\n",
      "51240/51240 [==============================] - 69s 1ms/step - loss: 0.7926 - accuracy: 0.7167\n",
      "Epoch 55/200\n",
      "51240/51240 [==============================] - 59s 1ms/step - loss: 0.7917 - accuracy: 0.7156\n",
      "Epoch 56/200\n",
      "51240/51240 [==============================] - 61s 1ms/step - loss: 0.8009 - accuracy: 0.7140\n",
      "Epoch 57/200\n",
      "51240/51240 [==============================] - 59s 1ms/step - loss: 0.7891 - accuracy: 0.7218\n",
      "Epoch 58/200\n",
      "51240/51240 [==============================] - 59s 1ms/step - loss: 0.7722 - accuracy: 0.7256\n",
      "Epoch 59/200\n",
      "51240/51240 [==============================] - 59s 1ms/step - loss: 0.7782 - accuracy: 0.7234\n",
      "Epoch 60/200\n",
      "51240/51240 [==============================] - 59s 1ms/step - loss: 0.7745 - accuracy: 0.7231\n",
      "Epoch 61/200\n",
      "51240/51240 [==============================] - 59s 1ms/step - loss: 0.7703 - accuracy: 0.7251\n",
      "Epoch 62/200\n",
      "51240/51240 [==============================] - 58s 1ms/step - loss: 0.7751 - accuracy: 0.7243\n",
      "Epoch 63/200\n",
      "51240/51240 [==============================] - 60s 1ms/step - loss: 0.7634 - accuracy: 0.7276\n",
      "Epoch 64/200\n",
      "51240/51240 [==============================] - 59s 1ms/step - loss: 0.7606 - accuracy: 0.7302\n",
      "Epoch 65/200\n",
      "51240/51240 [==============================] - 57s 1ms/step - loss: 0.7559 - accuracy: 0.7312\n",
      "Epoch 66/200\n",
      "51240/51240 [==============================] - 58s 1ms/step - loss: 0.7505 - accuracy: 0.7340\n",
      "Epoch 67/200\n",
      "51240/51240 [==============================] - 59s 1ms/step - loss: 0.7544 - accuracy: 0.7315\n",
      "Epoch 68/200\n",
      "51240/51240 [==============================] - 56s 1ms/step - loss: 0.7600 - accuracy: 0.7282\n",
      "Epoch 69/200\n",
      "51240/51240 [==============================] - 57s 1ms/step - loss: 0.7497 - accuracy: 0.7329\n",
      "Epoch 70/200\n",
      "51240/51240 [==============================] - 58s 1ms/step - loss: 0.7396 - accuracy: 0.7355\n",
      "Epoch 71/200\n",
      "51240/51240 [==============================] - 57s 1ms/step - loss: 0.7258 - accuracy: 0.7400\n",
      "Epoch 72/200\n",
      "51240/51240 [==============================] - 56s 1ms/step - loss: 0.7348 - accuracy: 0.7376\n",
      "Epoch 73/200\n",
      "51240/51240 [==============================] - 57s 1ms/step - loss: 0.7395 - accuracy: 0.7385\n",
      "Epoch 74/200\n",
      "51240/51240 [==============================] - 61s 1ms/step - loss: 0.7355 - accuracy: 0.7373\n",
      "Epoch 75/200\n",
      "51240/51240 [==============================] - 59s 1ms/step - loss: 0.7296 - accuracy: 0.7410\n",
      "Epoch 76/200\n",
      "51240/51240 [==============================] - 60s 1ms/step - loss: 0.7214 - accuracy: 0.7447\n",
      "Epoch 77/200\n",
      "51240/51240 [==============================] - 57s 1ms/step - loss: 0.7165 - accuracy: 0.7443\n",
      "Epoch 78/200\n",
      "51240/51240 [==============================] - 57s 1ms/step - loss: 0.7180 - accuracy: 0.7447\n",
      "Epoch 79/200\n",
      "51240/51240 [==============================] - 57s 1ms/step - loss: 0.7292 - accuracy: 0.7408\n",
      "Epoch 80/200\n",
      "51240/51240 [==============================] - 60s 1ms/step - loss: 0.7131 - accuracy: 0.7455\n",
      "Epoch 81/200\n",
      "51240/51240 [==============================] - 59s 1ms/step - loss: 0.7031 - accuracy: 0.7493\n",
      "Epoch 82/200\n",
      "51240/51240 [==============================] - 59s 1ms/step - loss: 0.7158 - accuracy: 0.7466\n",
      "Epoch 83/200\n",
      "51240/51240 [==============================] - 59s 1ms/step - loss: 0.7042 - accuracy: 0.7501\n",
      "Epoch 84/200\n",
      "51240/51240 [==============================] - 59s 1ms/step - loss: 0.7010 - accuracy: 0.7522\n",
      "Epoch 85/200\n",
      "51240/51240 [==============================] - 59s 1ms/step - loss: 0.6964 - accuracy: 0.7516\n",
      "Epoch 86/200\n",
      "51240/51240 [==============================] - 59s 1ms/step - loss: 0.7067 - accuracy: 0.7516\n",
      "Epoch 87/200\n",
      "51240/51240 [==============================] - 59s 1ms/step - loss: 0.6972 - accuracy: 0.7513\n",
      "Epoch 88/200\n",
      "51240/51240 [==============================] - 60s 1ms/step - loss: 0.6934 - accuracy: 0.7541\n",
      "Epoch 89/200\n",
      "51240/51240 [==============================] - 59s 1ms/step - loss: 0.7019 - accuracy: 0.7495\n",
      "Epoch 90/200\n",
      "51240/51240 [==============================] - 59s 1ms/step - loss: 0.6872 - accuracy: 0.7550\n",
      "Epoch 91/200\n",
      "51240/51240 [==============================] - 59s 1ms/step - loss: 0.6883 - accuracy: 0.7547\n",
      "Epoch 92/200\n",
      "51240/51240 [==============================] - 58s 1ms/step - loss: 0.6740 - accuracy: 0.7593\n",
      "Epoch 93/200\n",
      "51240/51240 [==============================] - 57s 1ms/step - loss: 0.6941 - accuracy: 0.7529\n",
      "Epoch 94/200\n",
      "51240/51240 [==============================] - 58s 1ms/step - loss: 0.6846 - accuracy: 0.7566\n",
      "Epoch 95/200\n",
      "51240/51240 [==============================] - 58s 1ms/step - loss: 0.6852 - accuracy: 0.7558\n",
      "Epoch 96/200\n",
      "51240/51240 [==============================] - 58s 1ms/step - loss: 0.6777 - accuracy: 0.7577\n",
      "Epoch 97/200\n",
      "51240/51240 [==============================] - 57s 1ms/step - loss: 0.6785 - accuracy: 0.7591\n",
      "Epoch 98/200\n",
      "51240/51240 [==============================] - 57s 1ms/step - loss: 0.6792 - accuracy: 0.7583\n",
      "Epoch 99/200\n",
      "51240/51240 [==============================] - 57s 1ms/step - loss: 0.6705 - accuracy: 0.7620\n",
      "Epoch 100/200\n",
      "51240/51240 [==============================] - 58s 1ms/step - loss: 0.6730 - accuracy: 0.7622\n",
      "Epoch 101/200\n",
      "51240/51240 [==============================] - 57s 1ms/step - loss: 0.6612 - accuracy: 0.7641\n",
      "Epoch 102/200\n",
      "51240/51240 [==============================] - 58s 1ms/step - loss: 0.6568 - accuracy: 0.7661\n",
      "Epoch 103/200\n",
      "51240/51240 [==============================] - 58s 1ms/step - loss: 0.6667 - accuracy: 0.7635\n",
      "Epoch 104/200\n",
      "51240/51240 [==============================] - 57s 1ms/step - loss: 0.6600 - accuracy: 0.7632\n",
      "Epoch 105/200\n",
      "51240/51240 [==============================] - 59s 1ms/step - loss: 0.6578 - accuracy: 0.7655\n",
      "Epoch 106/200\n",
      "51240/51240 [==============================] - 68s 1ms/step - loss: 0.6525 - accuracy: 0.7666\n",
      "Epoch 107/200\n",
      "51240/51240 [==============================] - 63s 1ms/step - loss: 0.6493 - accuracy: 0.7678\n",
      "Epoch 108/200\n",
      "51240/51240 [==============================] - 62s 1ms/step - loss: 0.6527 - accuracy: 0.7691\n",
      "Epoch 109/200\n",
      "51240/51240 [==============================] - 63s 1ms/step - loss: 0.6579 - accuracy: 0.7663\n",
      "Epoch 110/200\n",
      "51240/51240 [==============================] - 62s 1ms/step - loss: 0.6431 - accuracy: 0.7733\n",
      "Epoch 111/200\n",
      "51240/51240 [==============================] - 58s 1ms/step - loss: 0.6436 - accuracy: 0.7700\n",
      "Epoch 112/200\n",
      "51240/51240 [==============================] - 58s 1ms/step - loss: 0.6607 - accuracy: 0.7678\n",
      "Epoch 113/200\n",
      "51240/51240 [==============================] - 57s 1ms/step - loss: 0.6452 - accuracy: 0.7709\n",
      "Epoch 114/200\n",
      "51240/51240 [==============================] - 60s 1ms/step - loss: 0.6387 - accuracy: 0.7734\n",
      "Epoch 115/200\n",
      "51240/51240 [==============================] - 58s 1ms/step - loss: 0.6408 - accuracy: 0.7720\n",
      "Epoch 116/200\n",
      "51240/51240 [==============================] - 61s 1ms/step - loss: 0.6346 - accuracy: 0.7736\n",
      "Epoch 117/200\n",
      "51240/51240 [==============================] - 58s 1ms/step - loss: 0.6341 - accuracy: 0.7731\n",
      "Epoch 118/200\n",
      "51240/51240 [==============================] - 58s 1ms/step - loss: 0.6367 - accuracy: 0.7735\n",
      "Epoch 119/200\n",
      "51240/51240 [==============================] - 58s 1ms/step - loss: 0.6354 - accuracy: 0.7730\n",
      "Epoch 120/200\n",
      "51240/51240 [==============================] - 59s 1ms/step - loss: 0.6283 - accuracy: 0.7771\n",
      "Epoch 121/200\n",
      "51240/51240 [==============================] - 64s 1ms/step - loss: 0.6329 - accuracy: 0.7757\n",
      "Epoch 122/200\n",
      "51240/51240 [==============================] - 63s 1ms/step - loss: 0.6301 - accuracy: 0.7769\n",
      "Epoch 123/200\n",
      "51240/51240 [==============================] - 56s 1ms/step - loss: 0.6210 - accuracy: 0.7806\n",
      "Epoch 124/200\n",
      "51240/51240 [==============================] - 57s 1ms/step - loss: 0.6349 - accuracy: 0.7763\n",
      "Epoch 125/200\n",
      "51240/51240 [==============================] - 56s 1ms/step - loss: 0.6213 - accuracy: 0.7802\n",
      "Epoch 126/200\n",
      "51240/51240 [==============================] - 56s 1ms/step - loss: 0.6308 - accuracy: 0.7759\n",
      "Epoch 127/200\n",
      "51240/51240 [==============================] - 57s 1ms/step - loss: 0.6281 - accuracy: 0.7777\n",
      "Epoch 128/200\n",
      "51240/51240 [==============================] - 57s 1ms/step - loss: 0.6135 - accuracy: 0.7817\n",
      "Epoch 129/200\n",
      "51240/51240 [==============================] - 56s 1ms/step - loss: 0.6143 - accuracy: 0.7816\n",
      "Epoch 130/200\n",
      "51240/51240 [==============================] - 57s 1ms/step - loss: 0.6184 - accuracy: 0.7803\n",
      "Epoch 131/200\n",
      "51240/51240 [==============================] - 66s 1ms/step - loss: 0.6122 - accuracy: 0.7829\n",
      "Epoch 132/200\n",
      "51240/51240 [==============================] - 68s 1ms/step - loss: 0.6210 - accuracy: 0.7814\n",
      "Epoch 133/200\n",
      "51240/51240 [==============================] - 69s 1ms/step - loss: 0.6002 - accuracy: 0.7868\n",
      "Epoch 134/200\n",
      "51240/51240 [==============================] - 60s 1ms/step - loss: 0.6039 - accuracy: 0.7845\n",
      "Epoch 135/200\n",
      "51240/51240 [==============================] - 70s 1ms/step - loss: 0.6049 - accuracy: 0.7853\n",
      "Epoch 136/200\n",
      "51240/51240 [==============================] - 69s 1ms/step - loss: 0.6024 - accuracy: 0.7863\n",
      "Epoch 137/200\n",
      "51240/51240 [==============================] - 61s 1ms/step - loss: 0.6039 - accuracy: 0.7873\n",
      "Epoch 138/200\n",
      "51240/51240 [==============================] - 60s 1ms/step - loss: 0.6029 - accuracy: 0.7864\n",
      "Epoch 139/200\n",
      "51240/51240 [==============================] - 61s 1ms/step - loss: 0.5975 - accuracy: 0.7880\n",
      "Epoch 140/200\n",
      "51240/51240 [==============================] - 61s 1ms/step - loss: 0.6005 - accuracy: 0.7877\n",
      "Epoch 141/200\n",
      "51240/51240 [==============================] - 59s 1ms/step - loss: 0.6042 - accuracy: 0.7869\n",
      "Epoch 142/200\n",
      "51240/51240 [==============================] - 60s 1ms/step - loss: 0.5909 - accuracy: 0.7899\n",
      "Epoch 143/200\n",
      "51240/51240 [==============================] - 58s 1ms/step - loss: 0.5862 - accuracy: 0.7919\n",
      "Epoch 144/200\n",
      "51240/51240 [==============================] - 56s 1ms/step - loss: 0.5928 - accuracy: 0.7911\n",
      "Epoch 145/200\n",
      "51240/51240 [==============================] - 61s 1ms/step - loss: 0.5937 - accuracy: 0.7913\n",
      "Epoch 146/200\n",
      "51240/51240 [==============================] - 58s 1ms/step - loss: 0.5877 - accuracy: 0.7925\n",
      "Epoch 147/200\n",
      "51240/51240 [==============================] - 57s 1ms/step - loss: 0.5867 - accuracy: 0.7940\n",
      "Epoch 148/200\n",
      "51240/51240 [==============================] - 58s 1ms/step - loss: 0.5905 - accuracy: 0.7917\n",
      "Epoch 149/200\n",
      "51240/51240 [==============================] - 58s 1ms/step - loss: 0.5830 - accuracy: 0.7934\n",
      "Epoch 150/200\n",
      "51240/51240 [==============================] - 58s 1ms/step - loss: 0.5870 - accuracy: 0.7925\n",
      "Epoch 151/200\n",
      "51240/51240 [==============================] - 58s 1ms/step - loss: 0.5879 - accuracy: 0.7927\n",
      "Epoch 152/200\n",
      "51240/51240 [==============================] - 58s 1ms/step - loss: 0.5852 - accuracy: 0.7931\n",
      "Epoch 153/200\n",
      "51240/51240 [==============================] - 57s 1ms/step - loss: 0.5773 - accuracy: 0.7939\n",
      "Epoch 154/200\n",
      "51240/51240 [==============================] - 56s 1ms/step - loss: 0.5818 - accuracy: 0.7940\n",
      "Epoch 155/200\n",
      "51240/51240 [==============================] - 57s 1ms/step - loss: 0.5748 - accuracy: 0.7958\n",
      "Epoch 156/200\n",
      "51240/51240 [==============================] - 57s 1ms/step - loss: 0.5708 - accuracy: 0.7987\n",
      "Epoch 157/200\n",
      "51240/51240 [==============================] - 59s 1ms/step - loss: 0.5742 - accuracy: 0.7975\n",
      "Epoch 158/200\n",
      "51240/51240 [==============================] - 58s 1ms/step - loss: 0.5906 - accuracy: 0.7919\n",
      "Epoch 159/200\n",
      "51240/51240 [==============================] - 58s 1ms/step - loss: 0.5747 - accuracy: 0.7955\n",
      "Epoch 160/200\n",
      "51240/51240 [==============================] - 58s 1ms/step - loss: 0.5703 - accuracy: 0.7990\n",
      "Epoch 161/200\n",
      "51240/51240 [==============================] - 58s 1ms/step - loss: 0.5701 - accuracy: 0.7975\n",
      "Epoch 162/200\n",
      "51240/51240 [==============================] - 58s 1ms/step - loss: 0.5630 - accuracy: 0.8012\n",
      "Epoch 163/200\n",
      "51240/51240 [==============================] - 57s 1ms/step - loss: 0.5662 - accuracy: 0.7998\n",
      "Epoch 164/200\n",
      "51240/51240 [==============================] - 58s 1ms/step - loss: 0.5632 - accuracy: 0.8010\n",
      "Epoch 165/200\n",
      "51240/51240 [==============================] - 57s 1ms/step - loss: 0.5669 - accuracy: 0.8014\n",
      "Epoch 166/200\n",
      "51240/51240 [==============================] - 58s 1ms/step - loss: 0.5641 - accuracy: 0.8015\n",
      "Epoch 167/200\n",
      "51240/51240 [==============================] - 57s 1ms/step - loss: 0.5633 - accuracy: 0.8019\n",
      "Epoch 168/200\n",
      "51240/51240 [==============================] - 57s 1ms/step - loss: 0.5617 - accuracy: 0.8022\n",
      "Epoch 169/200\n",
      "51240/51240 [==============================] - 59s 1ms/step - loss: 0.5561 - accuracy: 0.8048\n",
      "Epoch 170/200\n",
      "51240/51240 [==============================] - 58s 1ms/step - loss: 0.5597 - accuracy: 0.8032\n",
      "Epoch 171/200\n",
      "51240/51240 [==============================] - 58s 1ms/step - loss: 0.5642 - accuracy: 0.8019\n",
      "Epoch 172/200\n",
      "51240/51240 [==============================] - 58s 1ms/step - loss: 0.5607 - accuracy: 0.8034\n",
      "Epoch 173/200\n",
      "51240/51240 [==============================] - 59s 1ms/step - loss: 0.5547 - accuracy: 0.8044\n",
      "Epoch 174/200\n",
      "51240/51240 [==============================] - 58s 1ms/step - loss: 0.5626 - accuracy: 0.8029\n",
      "Epoch 175/200\n",
      "51240/51240 [==============================] - 58s 1ms/step - loss: 0.5510 - accuracy: 0.8049\n",
      "Epoch 176/200\n",
      "51240/51240 [==============================] - 58s 1ms/step - loss: 0.5449 - accuracy: 0.8085\n",
      "Epoch 177/200\n",
      "51240/51240 [==============================] - 58s 1ms/step - loss: 0.5520 - accuracy: 0.8065\n",
      "Epoch 178/200\n",
      "51240/51240 [==============================] - 59s 1ms/step - loss: 0.5394 - accuracy: 0.8086\n",
      "Epoch 179/200\n",
      "51240/51240 [==============================] - 58s 1ms/step - loss: 0.5531 - accuracy: 0.8057\n",
      "Epoch 180/200\n",
      "51240/51240 [==============================] - 57s 1ms/step - loss: 0.5446 - accuracy: 0.8086\n",
      "Epoch 181/200\n",
      "51240/51240 [==============================] - 58s 1ms/step - loss: 0.5403 - accuracy: 0.8096\n",
      "Epoch 182/200\n",
      "51240/51240 [==============================] - 58s 1ms/step - loss: 0.5373 - accuracy: 0.8128\n",
      "Epoch 183/200\n",
      "51240/51240 [==============================] - 59s 1ms/step - loss: 0.5445 - accuracy: 0.8095\n",
      "Epoch 184/200\n",
      "51240/51240 [==============================] - 64s 1ms/step - loss: 0.5397 - accuracy: 0.8099\n",
      "Epoch 185/200\n",
      "51240/51240 [==============================] - 62s 1ms/step - loss: 0.5461 - accuracy: 0.8083\n",
      "Epoch 186/200\n",
      "51240/51240 [==============================] - 63s 1ms/step - loss: 0.5368 - accuracy: 0.8124\n",
      "Epoch 187/200\n",
      "51240/51240 [==============================] - 58s 1ms/step - loss: 0.5395 - accuracy: 0.8115\n",
      "Epoch 188/200\n",
      "51240/51240 [==============================] - 58s 1ms/step - loss: 0.5426 - accuracy: 0.8098\n",
      "Epoch 189/200\n",
      "51240/51240 [==============================] - 58s 1ms/step - loss: 0.5356 - accuracy: 0.8126\n",
      "Epoch 190/200\n",
      "51240/51240 [==============================] - 60s 1ms/step - loss: 0.5385 - accuracy: 0.8123\n",
      "Epoch 191/200\n",
      "51240/51240 [==============================] - 58s 1ms/step - loss: 0.5364 - accuracy: 0.8113\n",
      "Epoch 192/200\n",
      "51240/51240 [==============================] - 60s 1ms/step - loss: 0.5290 - accuracy: 0.8146\n",
      "Epoch 193/200\n",
      "51240/51240 [==============================] - 66s 1ms/step - loss: 0.5300 - accuracy: 0.8156\n",
      "Epoch 194/200\n",
      "51240/51240 [==============================] - 65s 1ms/step - loss: 0.5402 - accuracy: 0.8114\n",
      "Epoch 195/200\n",
      "51240/51240 [==============================] - 64s 1ms/step - loss: 0.5303 - accuracy: 0.8148\n",
      "Epoch 196/200\n",
      "51240/51240 [==============================] - 61s 1ms/step - loss: 0.5335 - accuracy: 0.8129\n",
      "Epoch 197/200\n",
      "51240/51240 [==============================] - 58s 1ms/step - loss: 0.5268 - accuracy: 0.8142\n",
      "Epoch 198/200\n",
      "51240/51240 [==============================] - 58s 1ms/step - loss: 0.5274 - accuracy: 0.8154\n",
      "Epoch 199/200\n",
      "51240/51240 [==============================] - 58s 1ms/step - loss: 0.5279 - accuracy: 0.8161\n",
      "Epoch 200/200\n",
      "51240/51240 [==============================] - 59s 1ms/step - loss: 0.5205 - accuracy: 0.8173\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1c50590f7b8>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7320/7320 [==============================] - 3s 420us/step\n",
      "Test loss: 0.5328557654867607\n",
      "Test accuracy: 0.8245901465415955\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
